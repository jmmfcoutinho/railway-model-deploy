{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "[1. Preserving your model](#1.-Preserving-your-model)   \n",
    "[2. Train your model](#2.-Train-your-model)   \n",
    "&emsp;[2.1 Let's build the pipeline](#2.1-Let's-build-the-pipeline)   \n",
    "&emsp;[2.2 Predicting on new observations](#2.2-Predicting-on-new-observations)   \n",
    "[3. Serialization of the necessary components](#3.-Serialization-of-the-necessary-components)   \n",
    "&emsp;[3.1 Serializing the columns in the correct order](#3.1-Serializing-the-columns-in-the-correct-order)   \n",
    "&emsp;[3.2 Serializing the fitted pipeline](#3.2-Serializing-the-fitted-pipeline)   \n",
    "&emsp;[3.3 Serializing the dtypes of the columns](#3.3-Serializing-the-dtypes-of-the-columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preserving your model\n",
    "\n",
    "In this learning unit you will learn how to preserve your model so that the value it generates can be used in a separate process or program than the one in which it was fitted. There are a few different ways to do this but for this specialization we will be using the following tools:\n",
    "\n",
    "1. [pickle](https://docs.python.org/3/library/pickle.html) from python core\n",
    "1. [pipelines](https://scikit-learn.org/stable/modules/compose.html#pipelines-and-composite-estimators) from scikit\n",
    "\n",
    "This will be our journey:\n",
    "\n",
    "1. **Train**: We are going to first train a model on the classic titanic dataset. We will use this one because it has categorical and numeric features, and missing values in both types.\n",
    "\n",
    "2. **Serialize**: Once the model has been trained as part of a pipeline, we will [serialize] https://en.wikipedia.org/wiki/Serialization) it using the [pickle](https://docs.python.org/3/library/pickle.html) package that is found in python's core.\n",
    "\n",
    "3. **Predict on new data**: After we are confident we can retrieve the pickled model from the disk, we will show how to prepare a brand new observation for prediction with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train your model\n",
    "\n",
    "Let's get started! We're not going to spend much time preparing the dataset or working on model performance\n",
    "because it's not the focus of this learning unit. So let's power through the first few steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read original dataset from disk and take a look at it\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500   NaN        S\n",
       "1         1       1  female  38.0      1      0  71.2833   C85        C\n",
       "2         1       3  female  26.0      0      0   7.9250   NaN        S\n",
       "3         1       1  female  35.0      1      0  53.1000  C123        S\n",
       "4         0       3    male  35.0      0      0   8.0500   NaN        S"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's get rid of a few features that don't hold anything\n",
    "# particularly useful and take another peak\n",
    "df = df.drop(['Ticket', 'Name', 'PassengerId'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's split it into X_train and y_train\n",
    "X_train, y_train = df.drop('Survived', axis=1), df.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Let's build the pipeline\n",
    "\n",
    "Okay, the next bit of necessary code isn't very much at all but\n",
    "is very dense. So let's take things one at a time to understand\n",
    "the motivation.\n",
    "\n",
    "We'll begin with just the model itself - a logistic regression\n",
    "and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert string to float: 'male'\n"
     ]
    }
   ],
   "source": [
    "# NOTE: We're using a try/except block here because (SPOILER ALERT)\n",
    "# the fit function is going to fail and we want the notebook to look clean.\n",
    "# Go ahead and remove the try/except block to see the error's stack trace\n",
    "\n",
    "try:\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "except ValueError as e:\n",
    "     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know this game - scikit classifiers don't know how to deal\n",
    "with non-numerical data. Since we already know about pipelines,\n",
    "let's try to put together a pipeline that has a OneHotEncoder\n",
    "in an attempt to deal with the non-numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape mismatch: if categories is an array, it has to be of shape (n_features,).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pipeline = make_pipeline(\n",
    "        OneHotEncoder(\n",
    "            categories=list(\n",
    "                X_train.select_dtypes(include=['object']).columns)),\n",
    "        LogisticRegression()\n",
    "    )\n",
    "    pipeline.fit(X_train, y_train)\n",
    "except ValueError as e:\n",
    "     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "from scikit can't deal with categories with missing values. We could replace these missing values with a new category (like `unknown`), but it's easier to use the [category_encoders](http://contrib.scikit-learn.org/category_encoders/index.html) package, that deals with this out of the box.\n",
    "\n",
    "We'll use the [OneHotEncoder](http://contrib.scikit-learn.org/category_encoders/onehot.html). This one is a bit smarter than scikit's one, as the default behavior is for all string columns to be dummified.\n",
    "\n",
    "One last thing to note is the usage of the `handle_unknown` keyword, which is set to `value`. This is the default behaviour, but it's worth understanding what is happening. Later on, at prediction time, if the encoder runs into a value that it hasn't seen before, it won't throw an error. Instead, all the dummy features that were created will have value `0` for this observation.\n",
    "It's not a perfect solution to be silently ignoring stuff that you haven't seen before but hey, it's better than crashing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X contains NaN.\n",
      "LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pipeline = make_pipeline(\n",
    "        category_encoders.OneHotEncoder(handle_unknown='value'),\n",
    "        LogisticRegression(),\n",
    "    )\n",
    "    pipeline.fit(X_train, y_train)\n",
    "except ValueError as e:\n",
    "     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ups, we're still having the same error... \n",
    "We've dealt with NaNs in the categorical columns, but some of the numeric columns still have NaNs in them. Now we need to use the [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n",
    "for those bad boys. SimpleImputer fills in missing values in the data. By default, it assumes that the missing values are NaNs, but we could specify other types of missing values with the `missing_values` keyword. With the `strategy` keyword we define how to fill in the missing values: in this case, we're replacing them with the mean value of the feature where the missing value occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;onehotencoder&#x27;,\n",
       "                 OneHotEncoder(cols=[&#x27;Sex&#x27;, &#x27;Cabin&#x27;, &#x27;Embarked&#x27;])),\n",
       "                (&#x27;simpleimputer&#x27;, SimpleImputer()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;onehotencoder&#x27;,\n",
       "                 OneHotEncoder(cols=[&#x27;Sex&#x27;, &#x27;Cabin&#x27;, &#x27;Embarked&#x27;])),\n",
       "                (&#x27;simpleimputer&#x27;, SimpleImputer()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;Sex&#x27;, &#x27;Cabin&#x27;, &#x27;Embarked&#x27;])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('onehotencoder',\n",
       "                 OneHotEncoder(cols=['Sex', 'Cabin', 'Embarked'])),\n",
       "                ('simpleimputer', SimpleImputer()),\n",
       "                ('logisticregression', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    category_encoders.OneHotEncoder(),\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    LogisticRegression(max_iter=1000),\n",
    ")\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! Now we are using a pipeline and training a classifier without doing\n",
    "any explicit preprocessing of the dataset!\n",
    "\n",
    "With the pipeline we should now be able to move on with our lives and start\n",
    "processing new observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Predicting on new observations\n",
    "\n",
    "Let's construct a new observation using a protocol that is technology agnostic: [json](https://en.wikipedia.org/wiki/JSON).\n",
    "\n",
    "We will assume that a new observation has come over the wire using a transportation\n",
    "layer such as HTTP, which means that it will arrive to us as a json string (a string whose content follows the json protocol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs_str = '{\"Age\": 22.0, \"Cabin\": null, \"Embarked\": \"S\", \"Fare\": 7.25, \"Parch\": 0, \"Pclass\": 3, \"Sex\": \"male\", \"SibSp\": 1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we've got a new observation as a json string. This is desirable because no matter what\n",
    "programming language or environment we are in, we know that there will be support for deserialization\n",
    "into a native type. In ruby these are hashes, in javascript they are objects, and in python they are\n",
    "dictionaries.\n",
    "\n",
    "So let's turn our json string into a dictionary - it's a great starting point to do anything we may need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "new_obs_dict = json.loads(new_obs_str)\n",
    "print('type {}'.format(type(new_obs_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': 22.0,\n",
       " 'Cabin': None,\n",
       " 'Embarked': 'S',\n",
       " 'Fare': 7.25,\n",
       " 'Parch': 0,\n",
       " 'Pclass': 3,\n",
       " 'Sex': 'male',\n",
       " 'SibSp': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_obs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so fast... scikit models don't know how to deal with dictionaries!\n",
    "\n",
    "Well, we know that when we trained the model, the pipeline took a pandas\n",
    "dataframe so that's what we should be passing into `predict_proba` as well.\n",
    "\n",
    "With that in mind, let's take a few lines of code to transform the dictionary\n",
    "into a pandas dataframe. Note that a series isn't good enough, it must be\n",
    "a full dataframe, even if it's just for a single observation.\n",
    "\n",
    "Although it's only a few lines of code, it's pretty dense, so be sure to\n",
    "read the comments in order to fully understand what's going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to create a dataframe with the columns in the correct\n",
    "# order. You can get the correct order by getting the columns from\n",
    "# the X_train dataframe with which the model was trained. Doing this\n",
    "# will preserve the correct order.\n",
    "\n",
    "# Also note that you must pass the dictionary as an entry\n",
    "# in a list, even if there is only a single one... scikit models\n",
    "# always assume things are being processed in batches.\n",
    "obs = pd.DataFrame([new_obs_dict], columns=X_train.columns.tolist())\n",
    "\n",
    "# Now you need to make sure that the types are correct so that the\n",
    "# pipeline steps will have things as expected.\n",
    "obs = obs.astype(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Sex   Age  SibSp  Parch  Fare Cabin Embarked\n",
       "0       3  male  22.0      1      0  7.25  None        S"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90829658, 0.09170342]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally you can call predict_proba and the pipeline will function\n",
    "# as expected, outputting probabilities of the negative and positive class (dead or alive in this case).\n",
    "pipeline.predict_proba(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we are feeling pretty cool right now. We have a trained model, we can process new observations! Now that we have this under control, let's take a look at preserving our model so that we can keep this sweetness for posterity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Serialization of the necessary components\n",
    "\n",
    "Okay let's take a moment to imagine that we will need to use this model in a totally different environment than the one it was trained in.\n",
    "\n",
    "For instance, if we want to share the model with a friend: we woulnd't want them to retrain it (some models take  hours to train), and we wounldn't want to have to send them the training data (it might be very large and it might be confidential data that is not suitable to be shared).\n",
    "\n",
    "Even within the same computer, we may want to use the model in a different notebook than the one it was trained in. Or in a different python process. Or in a flask server (hint hint).\n",
    "\n",
    "What we  want to do is to save the model to the disk so that it can be transfered somewhere else and used later on.\n",
    "\n",
    "Remember that serialization is just the process of storing something so that it can be deserialized and used later  on. Let's think about what we need in order to be able to call `predict_proba` on a new observation. It is:\n",
    "\n",
    "1. The column names in the correct order\n",
    "1. The fitted pipeline\n",
    "1. The dtypes of the columns of the training set\n",
    "\n",
    "One at a time, let's look at serializing these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Serializing the columns in the correct order\n",
    "\n",
    "Probably the most well-known serialization format for data is json.\n",
    "\n",
    "This is great because it's robust and technology agnostic. Let's serialize the columns of the training set in the correct order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('columns.json', 'w') as fh:\n",
    "    json.dump(X_train.columns.tolist(), fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Serializing the fitted pipeline\n",
    "\n",
    "Now we need to serialize the fitted pipeline. Unfortunately we don't have something as clean as json for this since the pipeline is a python object and not just raw data (like the column names are).\n",
    "\n",
    "In order to preserve the pipeline, we will need to use a library to export a python object (the fitted pipeline) into our hard drive in such a way that python can reload it again. The most common library for serialization of python objects is [pickle](https://docs.python.org/3/library/pickle.html), which is part of the core python language (the standard library). However scikit-learn comes with a version of pickle designed to save scikit-learn estimators, [joblib](http://scikit-learn.org/stable/modules/model_persistence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline.pickle']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(pipeline, 'pipeline.pickle') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**. One thing to consider when serializing/deserializing scikit-learn estimators is that joblib/pickle don't store the definitions of the estimators (the code). This means 2 things:\n",
    "\n",
    "- All of the libraries you use to build the pipeline on your laptop need to be available (installed) in the machine where you deploy it. \n",
    "\n",
    "- All of the custom code you use (for example, any custom transformer you build) in the pipeline needs to be defined as well when you load the pipeline back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Serializing the dtypes of the columns\n",
    "\n",
    "We have a similar situation with the dtypes. When you call `X_train.dtypes`, you will get a list of python\n",
    "objects, so we have to use pickle to serialize them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dtypes.pickle', 'wb') as fh:\n",
    "    pickle.dump(X_train.dtypes, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alrighty then, we have now serialized all the necessary components\n",
    "of our model in order to be able to process new observations!\n",
    "\n",
    "Move on to the next notebook to see how we can deserialize and use\n",
    "all of this work in a completely different process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
